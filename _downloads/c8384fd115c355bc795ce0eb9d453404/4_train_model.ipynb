{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training Pipeline\n\nThis example shows the complete training pipeline from raw data to model training.\nEach step is explained and demonstrated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport torch\n\n# Get the path to the data file\n# Find data directory relative to myoverse package (works in all contexts)\nimport myoverse\n_pkg_dir = Path(myoverse.__file__).parent.parent\nDATA_DIR = _pkg_dir / \"examples\" / \"data\"\nif not DATA_DIR.exists():\n    DATA_DIR = Path.cwd() / \"examples\" / \"data\"\n\n# Determine device\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")\n\n# Step 1: Create Dataset with Preprocessing\n# ------------------------------------------\n# Use DatasetCreator with Modality transforms for pre-storage processing.\n# Here we use the EMBC paper configuration as an example.\n\nfrom myoverse.datasets import DatasetCreator, Modality, embc_kinematics_transform\n\nprint(\"=\" * 60)\nprint(\"STEP 1: Dataset Creation\")\nprint(\"=\" * 60)\n\ncreator = DatasetCreator(\n    modalities={\n        # EMG: raw continuous data (320 channels from 5 electrode grids)\n        \"emg\": Modality(\n            path=DATA_DIR / \"emg.pkl\",\n            dims=(\"channel\", \"time\"),\n        ),\n        # Kinematics: apply transform to flatten and remove wrist\n        # (21, 3, time) -> (60, time)\n        \"kinematics\": Modality(\n            path=DATA_DIR / \"kinematics.pkl\",\n            dims=(\"dof\", \"time\"),\n            transform=embc_kinematics_transform(),\n        ),\n    },\n    sampling_frequency=2048.0,\n    tasks_to_use=[\"1\", \"2\"],\n    save_path=DATA_DIR / \"tutorial_dataset.zip\",\n    test_ratio=0.2,\n    val_ratio=0.2,\n    debug_level=1,\n)\ncreator.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define Training Transforms\nTransforms are applied on-the-fly during training.\n- embc_train_transform: Creates dual representation (raw + lowpass) + augmentation\n- embc_eval_transform: Same processing without augmentation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from myoverse.datasets import embc_eval_transform, embc_target_transform, embc_train_transform\n\nprint()\nprint(\"=\" * 60)\nprint(\"STEP 2: Training Transforms\")\nprint(\"=\" * 60)\n\n# Training: dual representation + noise augmentation\ntrain_tf = embc_train_transform(augmentation=\"noise\")\nprint(f\"Train transform: {train_tf}\")\n\n# Validation: dual representation only (no augmentation)\nval_tf = embc_eval_transform()\nprint(f\"Val transform: {val_tf}\")\n\n# Target: average kinematics over window -> single prediction per DOF\ntarget_tf = embc_target_transform()\nprint(f\"Target transform: {target_tf}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create DataModule\nDataModule handles:\n- Loading from zarr directly to tensors (GPU if available)\n- On-the-fly windowing (no pre-chunking needed)\n- Input/target selection (decided at training time, not storage time)\n- Transform application\n- Batching and DataLoader creation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from myoverse.datasets import DataModule\n\nprint()\nprint(\"=\" * 60)\nprint(\"STEP 3: DataModule Setup\")\nprint(\"=\" * 60)\n\ndm = DataModule(\n    data_path=DATA_DIR / \"tutorial_dataset.zip\",\n    # Select which modalities are inputs vs targets\n    inputs=[\"emg\"],\n    targets=[\"kinematics\"],\n    # Windowing parameters\n    window_size=192,  # ~94ms at 2048Hz\n    window_stride=64,  # For val/test (deterministic sliding window)\n    n_windows_per_epoch=500,  # For training (random positions) - small for demo\n    # Transforms (applied on-the-fly)\n    train_transform=train_tf,\n    val_transform=val_tf,\n    target_transform=target_tf,  # Average kinematics over window\n    # DataLoader settings\n    batch_size=32,\n    num_workers=0,  # Set to 4+ for parallel loading\n    # Device: load directly to GPU if available\n    device=DEVICE,\n)\n\n# Setup creates the datasets\ndm.setup(\"fit\")\n\nprint(f\"Training samples per epoch: {len(dm.train_dataloader()) * dm.batch_size}\")\nprint(f\"Validation batches: {len(dm.val_dataloader())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Inspect Batch Structure\nWith single input/target, DataModule returns tensors directly\n(for compatibility with existing models).\nWith multiple inputs/targets, it returns dicts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print()\nprint(\"=\" * 60)\nprint(\"STEP 4: Batch Structure\")\nprint(\"=\" * 60)\n\nbatch = next(iter(dm.train_dataloader()))\nemg_batch, kin_batch = batch\n\nprint(f\"EMG input shape: {emg_batch.shape}\")\nprint(f\"EMG input device: {emg_batch.device}\")\nprint(f\"Kinematics target shape: {kin_batch.shape}\")\n\n# EMG shape explanation (with Stack transform):\n# - Batch size: 32\n# - Representations: 2 (raw, filtered)\n# - Channels: varies based on data\n# - Time: 192 samples\nprint()\nprint(\"EMG shape = (batch, representation, channel, time) via Stack transform\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Model\nRaulNetV16 expects:\n- Input: (batch, 2, channels, time) - 2 representations\n- Output: (batch, 60) - 60 DOF predictions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print()\nprint(\"=\" * 60)\nprint(\"STEP 5: Model Setup\")\nprint(\"=\" * 60)\n\nfrom myoverse.models import RaulNetV16\n\n# Get actual channel count from data\nn_channels = emg_batch.shape[2]\nn_grids = n_channels // 64 if n_channels >= 64 else 1\n\nmodel = RaulNetV16(\n    learning_rate=1e-4,\n    nr_of_input_channels=2,  # raw + filtered\n    input_length__samples=192,\n    nr_of_outputs=60,  # 60 DOF\n    nr_of_electrode_grids=n_grids,\n    nr_of_electrodes_per_grid=64,\n    cnn_encoder_channels=(4, 1, 1),\n    mlp_encoder_channels=(8, 8),\n    event_search_kernel_length=31,\n    event_search_kernel_stride=8,\n)\n\n# Move model to same device as data\nmodel = model.to(DEVICE)\n\nprint(f\"Model: {model.__class__.__name__}\")\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Forward Pass Test\nNamed tensors are used during transforms for dimension-awareness,\nbut stripped in the collate function before passing to the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print()\nprint(\"=\" * 60)\nprint(\"STEP 6: Forward Pass Test\")\nprint(\"=\" * 60)\n\nprint(f\"DataModule output: {emg_batch.shape}\")\nprint(\"(Names stripped in collate - ready for model)\")\n\n# Quick forward pass test\nmodel.eval()\nwith torch.no_grad():\n    output = model(emg_batch)\nprint(f\"Model output: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Training Loop\nUse PyTorch Lightning Trainer for training.\nNote: For real training, increase max_epochs and n_windows_per_epoch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import lightning as L\n\nprint()\nprint(\"=\" * 60)\nprint(\"STEP 7: Training (1 epoch, 500 windows)\")\nprint(\"=\" * 60)\n\ntorch.set_float32_matmul_precision(\"medium\")  # For performance on some CPUs\n\ntrainer = L.Trainer(\n    accelerator=\"auto\",\n    devices=1,\n    precision=\"32\",  # Use 32-bit for CPU compatibility\n    max_epochs=1,\n    log_every_n_steps=5,\n    logger=False,\n    enable_checkpointing=False,\n    enable_progress_bar=True,\n)\n\n# Train for 1 epoch\ntrainer.fit(model, datamodule=dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nThe complete pipeline:\n\n1. **DatasetCreator** - Store continuous data with pre-processing transforms\n2. **Modality.transform** - Pre-storage transforms (e.g., flatten kinematics)\n3. **DataModule** - Load directly to GPU, window, select inputs/targets\n4. **train_transform** - On-the-fly transforms (filtering, augmentation)\n5. **Model** - Your neural network\n\nKey benefits:\n- Modular: swap transforms without changing dataset\n- Efficient: zarr -> GPU loading (kvikio if available)\n- Flexible: input/target selection at training time\n- Named tensors: dimension-aware transforms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print()\nprint(\"=\" * 60)\nprint(\"PIPELINE COMPLETE\")\nprint(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}